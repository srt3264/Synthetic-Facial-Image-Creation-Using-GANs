{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment:\n",
    "!pip install transformers\n",
    "!pip install torch torchvision\n",
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# Load the pre-trained ViT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data:\n",
    "data_dir = \"archive/\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_dataset = datasets.ImageFolder(data_dir, data_transforms)\n",
    "train_size = int(0.8 * len(image_dataset))\n",
    "val_size = len(image_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(image_dataset, [train_size, val_size])\n",
    "\n",
    "train_hf_dataset = Dataset.from_dict({\"image\": [x[0] for x in train_dataset], \"label\": [x[1] for x in train_dataset]})\n",
    "val_hf_dataset = Dataset.from_dict({\"image\": [x[0] for x in val_dataset], \"label\": [x[1] for x in val_dataset]})\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ViT model\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor, TrainingArguments, Trainer\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "model = ViTForImageClassification.from_pretrained(model_name, num_labels=7)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset using the feature_extractor:\n",
    "def preprocess_images(dataset):\n",
    "    def preprocess_function(examples):\n",
    "        images = examples[\"image\"]\n",
    "        images = [torch.squeeze(image).permute(1, 2, 0).numpy() for image in images]\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        return {\"pixel_values\": inputs[\"pixel_values\"], \"labels\": examples[\"label\"]}\n",
    "    return dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "train_hf_dataset = preprocess_images(train_hf_dataset)\n",
    "val_hf_dataset = preprocess_images(val_hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hf_dataset,\n",
    "    eval_dataset=val_hf_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the model to huggingface\n",
    "# from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
