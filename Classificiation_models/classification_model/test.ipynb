{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "happy\n",
      "disgust\n",
      "neutral\n",
      "angry\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "happy\n",
      "disgust\n",
      "neutral\n",
      "angry\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print the sub folder name in archive folder\n",
    "for root, dirs, files in os.walk(\"archive\"):\n",
    "    for name in dirs:\n",
    "        print(os.path.join(name))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the FER-2013 dataset: The FER-2013 dataset is available for download from Kaggle and other sources. It contains 35,887 grayscale images of faces labeled with one of seven emotions (angry, disgust, fear, happy, sad, surprise, neutral).  \n",
    "\n",
    "Prepare the dataset: You'll need to preprocess the images to prepare them for training. This may involve resizing, cropping, and normalization.  \n",
    "\n",
    "Build the model: There are several deep learning frameworks that you can use to build an emotion classification model, such as TensorFlow, PyTorch, or Keras. You can choose to build your own custom model architecture or use a pre-trained model like VGG, ResNet, or Inception.\n",
    "  \n",
    "  Train the model: Split the dataset into training and validation sets and use the training data to train the model. Monitor the model's performance on the validation set to prevent overfitting.\n",
    "  \n",
    "  Evaluate the model: Use the test set to evaluate the model's performance on unseen data. Calculate metrics such as accuracy, precision, recall, and F1 score to measure the model's performance.\n",
    "  \n",
    "  Fine-tune the model: If the model is not performing well enough, you can try fine-tuning the model by adjusting the model architecture, hyperparameters, or training data.\n",
    "  \n",
    "  Deploy the model: Once you're satisfied with the model's performance, you can deploy it in a production environment to classify emotions in real-time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> resizing, cropping, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# def preprocess_image(image_path, target_size):\n",
    "#     \"\"\"\n",
    "#     Preprocess a JPEG image for training an emotion classification model.\n",
    "\n",
    "#     Parameters:\n",
    "#         image_path (str): Path to the JPEG image file.\n",
    "#         target_size (tuple): Desired size of the preprocessed image.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Preprocessed image as a NumPy array.\n",
    "#     \"\"\"\n",
    "#     # Load the image using OpenCV.\n",
    "#     image = cv2.imread(image_path)\n",
    "\n",
    "#     # Convert the image to grayscale.\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#     # Resize the image to a larger size while maintaining aspect ratio.\n",
    "#     height, width = image.shape\n",
    "#     if width > height:\n",
    "#         scale = target_size[0] / width\n",
    "#     else:\n",
    "#         scale = target_size[1] / height\n",
    "#     image = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "\n",
    "#     # Crop the center of the resized image to the target size.\n",
    "#     height, width = image.shape\n",
    "#     y1 = (height - target_size[1]) // 2\n",
    "#     y2 = y1 + target_size[1]\n",
    "#     x1 = (width - target_size[0]) // 2\n",
    "#     x2 = x1 + target_size[0]\n",
    "#     image = image[y1:y2, x1:x2]\n",
    "\n",
    "#     # Normalize the pixel values to the range [-1, 1].\n",
    "#     image = (image / 255.0) * 2.0 - 1.0\n",
    "\n",
    "#     # Convert the image to a NumPy array and add a batch dimension.\n",
    "#     image = np.expand_dims(image, axis=0)\n",
    "\n",
    "#     return image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having error importing cv2 :sudo apt-get update /\n",
    "sudo apt-get install libgl1-mesa-glx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test preprocess_image on training image\n",
    "# image = preprocess_image('archive/train/angry/Training_3908.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> train our own model using the VGG architecture,This approach allows you to benefit from the already learned features by the VGG model and adapt it to specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 20:34:41.481543: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-05 20:34:43.907060: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-05 20:34:43.908050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 20:34:48.180286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-04-05 20:34:54.974018: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# older version of tensorflow code\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications import VGG16\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # Load the VGG16 model without the top layers\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# # Freeze the base model layers (optional)\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add custom layers on top of the base model\n",
    "# x = base_model.output\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# output_layer = Dense(7, activation='softmax')(x)  # Change to 7 classes for emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the final model\n",
    "# model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam('learning_rate'==1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Create data generators for training and validation data\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 17:52:54.397630: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - ETA: 0s - loss: 2.1975 - accuracy: 0.1693"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 18:35:49.311196: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 3178s 4s/step - loss: 2.1975 - accuracy: 0.1693 - val_loss: 2.0477 - val_accuracy: 0.1719\n",
      "Epoch 2/50\n",
      " 61/898 [=>............................] - ETA: 39:46 - loss: 2.2230 - accuracy: 0.1603"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m valid_generator \u001b[39m=\u001b[39m valid_datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[1;32m     13\u001b[0m     valid_data_dir,\n\u001b[1;32m     14\u001b[0m     target_size\u001b[39m=\u001b[39m(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m),\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,  \u001b[39m# Change to your desired batch size\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     classes\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msad\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msurprise\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhappy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mangry\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneutral\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdisgust\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     21\u001b[0m     train_generator,\n\u001b[1;32m     22\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,  \u001b[39m# Change to your desired number of epochs\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalid_generator,\n\u001b[1;32m     24\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_generator),\n\u001b[1;32m     25\u001b[0m     validation_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(valid_generator))\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create data generators for training and validation data\n",
    "train_data_dir = 'archive/train/'\n",
    "valid_data_dir = 'archive/test/'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,  # Change to your desired batch size\n",
    "    class_mode='categorical',\n",
    "    classes=['fear', 'sad', 'surprise', 'happy', 'angry', 'neutral', 'disgust'])\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,  # Change to your desired batch size\n",
    "    class_mode='categorical',\n",
    "    classes=['fear', 'sad', 'surprise', 'happy', 'angry', 'neutral', 'disgust'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Change to your desired number of epochs\n",
    "    validation_data=valid_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(valid_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this. is for the GPU lspci | grep -i vga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.7288 Acc: 0.3029\n",
      "Epoch 1/10 - Loss: 1.6131 Acc: 0.3731\n",
      "Epoch 2/10 - Loss: 1.6556 Acc: 0.3445\n",
      "Epoch 2/10 - Loss: 1.5889 Acc: 0.3885\n",
      "Epoch 3/10 - Loss: 1.6217 Acc: 0.3639\n",
      "Epoch 3/10 - Loss: 1.5781 Acc: 0.3919\n",
      "Epoch 4/10 - Loss: 1.6136 Acc: 0.3676\n",
      "Epoch 4/10 - Loss: 1.5657 Acc: 0.3953\n",
      "Epoch 5/10 - Loss: 1.6054 Acc: 0.3687\n",
      "Epoch 5/10 - Loss: 1.5545 Acc: 0.4020\n",
      "Epoch 6/10 - Loss: 1.5972 Acc: 0.3761\n",
      "Epoch 6/10 - Loss: 1.5410 Acc: 0.4022\n",
      "Epoch 7/10 - Loss: 1.5925 Acc: 0.3777\n",
      "Epoch 7/10 - Loss: 1.5436 Acc: 0.4050\n",
      "Epoch 8/10 - Loss: 1.5867 Acc: 0.3806\n",
      "Epoch 8/10 - Loss: 1.5321 Acc: 0.4022\n",
      "Epoch 9/10 - Loss: 1.5838 Acc: 0.3836\n",
      "Epoch 9/10 - Loss: 1.5292 Acc: 0.4119\n",
      "Epoch 10/10 - Loss: 1.5701 Acc: 0.3840\n",
      "Epoch 10/10 - Loss: 1.5332 Acc: 0.4025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MobileNetV2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Variables\n",
    "train_folder = 'train'\n",
    "num_classes = 7\n",
    "image_size = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(0, shear=0.2, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(train_folder, transform=data_transforms)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load pre-trained MobileNetV2\n",
    "mobile_base = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze base layers\n",
    "for param in mobile_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Custom top layers\n",
    "mobile_base.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(1280, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mobile_base.to(device)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(mobile_base.classifier[1].parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    mobile_base.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobile_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    mobile_base.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobile_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / val_size\n",
    "    epoch_acc = running_corrects.double() / val_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    # Save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        torch.save(mobile_base.state_dict(), 'best_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/codespace/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:01<00:00, 309MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.7913 Acc: 0.2821\n",
      "Epoch 1/10 - Loss: 1.6426 Acc: 0.3524\n",
      "Epoch 2/10 - Loss: 1.7074 Acc: 0.3227\n",
      "Epoch 2/10 - Loss: 1.6226 Acc: 0.3608\n",
      "Epoch 3/10 - Loss: 1.6833 Acc: 0.3360\n",
      "Epoch 3/10 - Loss: 1.6005 Acc: 0.3724\n",
      "Epoch 4/10 - Loss: 1.6658 Acc: 0.3409\n",
      "Epoch 4/10 - Loss: 1.6119 Acc: 0.3728\n",
      "Epoch 5/10 - Loss: 1.6570 Acc: 0.3476\n",
      "Epoch 5/10 - Loss: 1.5837 Acc: 0.3781\n",
      "Epoch 6/10 - Loss: 1.6490 Acc: 0.3521\n",
      "Epoch 6/10 - Loss: 1.5955 Acc: 0.3829\n",
      "Epoch 7/10 - Loss: 1.6509 Acc: 0.3502\n",
      "Epoch 7/10 - Loss: 1.5862 Acc: 0.3800\n",
      "Epoch 8/10 - Loss: 1.6432 Acc: 0.3515\n",
      "Epoch 8/10 - Loss: 1.5776 Acc: 0.3901\n",
      "Epoch 9/10 - Loss: 1.6454 Acc: 0.3509\n",
      "Epoch 9/10 - Loss: 1.5766 Acc: 0.3875\n",
      "Epoch 10/10 - Loss: 1.6412 Acc: 0.3545\n",
      "Epoch 10/10 - Loss: 1.5788 Acc: 0.3898\n"
     ]
    }
   ],
   "source": [
    "# VGG16\n",
    "import os\n",
    "import numpy as np  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Variables\n",
    "train_folder = 'train'\n",
    "num_classes = 7\n",
    "image_size = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(0, shear=0.2, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(train_folder, transform=data_transforms)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "vgg_base = models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze base layers\n",
    "for param in vgg_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Custom top layers\n",
    "vgg_base.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_base.to(device)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(vgg_base.classifier[6].parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    vgg_base.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    vgg_base.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = vgg_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / val_size\n",
    "    epoch_acc = running_corrects.double() / val_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    # Save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        torch.save(vgg_base.state_dict(), 'best_model_vgg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/codespace/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 241MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.7239 Acc: 0.3060\n",
      "Epoch 1/10 - Loss: 1.6448 Acc: 0.3646\n",
      "Epoch 2/10 - Loss: 1.6307 Acc: 0.3586\n",
      "Epoch 2/10 - Loss: 1.6092 Acc: 0.3767\n",
      "Epoch 3/10 - Loss: 1.6047 Acc: 0.3710\n",
      "Epoch 3/10 - Loss: 1.5842 Acc: 0.3884\n",
      "Epoch 4/10 - Loss: 1.5887 Acc: 0.3750\n",
      "Epoch 4/10 - Loss: 1.5771 Acc: 0.3921\n",
      "Epoch 5/10 - Loss: 1.5715 Acc: 0.3853\n",
      "Epoch 5/10 - Loss: 1.5759 Acc: 0.3884\n",
      "Epoch 6/10 - Loss: 1.5692 Acc: 0.3887\n",
      "Epoch 6/10 - Loss: 1.5580 Acc: 0.4022\n",
      "Epoch 7/10 - Loss: 1.5674 Acc: 0.3889\n",
      "Epoch 7/10 - Loss: 1.5614 Acc: 0.3977\n",
      "Epoch 8/10 - Loss: 1.5613 Acc: 0.3912\n",
      "Epoch 8/10 - Loss: 1.5630 Acc: 0.3892\n",
      "Epoch 9/10 - Loss: 1.5584 Acc: 0.3933\n",
      "Epoch 9/10 - Loss: 1.5533 Acc: 0.4039\n",
      "Epoch 10/10 - Loss: 1.5547 Acc: 0.3928\n",
      "Epoch 10/10 - Loss: 1.5495 Acc: 0.3977\n"
     ]
    }
   ],
   "source": [
    "# ResNet50\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Variables\n",
    "train_folder = 'train'\n",
    "num_classes = 7\n",
    "image_size = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(0, shear=0.2, scale=(0.8, 1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(train_folder, transform=data_transforms)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "resnet_base = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze base layers\n",
    "for param in resnet_base.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Custom top layers\n",
    "resnet_base.fc = nn.Sequential(\n",
    "    nn.Linear(2048, num_classes),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_base.to(device)\n",
    "\n",
    "# Set up the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(resnet_base.fc.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    resnet_base.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    resnet_base.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet_base(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / val_size\n",
    "    epoch_acc = running_corrects.double() / val_size\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    # Save the model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        torch.save(resnet_base.state_dict(), 'best_model_resnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:15:52.646440: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-05 21:15:55.079535: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-05 21:15:55.080910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-05 21:15:59.097747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22607 images belonging to 7 classes.\n",
      "Found 5649 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:16:07.388266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:16:09.668342: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-05 21:17:44.952972: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25142, saving model to best_model.h5\n",
      "706/706 - 118s - loss: 1.9773 - accuracy: 0.2501 - val_loss: 1.7692 - val_accuracy: 0.2514 - 118s/epoch - 168ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.25142\n",
      "706/706 - 109s - loss: 1.7956 - accuracy: 0.2511 - val_loss: 1.7580 - val_accuracy: 0.2512 - 109s/epoch - 155ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.25142\n",
      "706/706 - 109s - loss: 1.7770 - accuracy: 0.2512 - val_loss: 1.7565 - val_accuracy: 0.2514 - 109s/epoch - 155ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.25142\n",
      "706/706 - 109s - loss: 1.7714 - accuracy: 0.2512 - val_loss: 1.7371 - val_accuracy: 0.2514 - 109s/epoch - 155ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.25142 to 0.25195, saving model to best_model.h5\n",
      "706/706 - 110s - loss: 1.7659 - accuracy: 0.2510 - val_loss: 1.7144 - val_accuracy: 0.2520 - 110s/epoch - 155ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.25195\n",
      "706/706 - 114s - loss: 1.7614 - accuracy: 0.2509 - val_loss: 1.7200 - val_accuracy: 0.2516 - 114s/epoch - 161ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.25195\n",
      "706/706 - 112s - loss: 1.7620 - accuracy: 0.2511 - val_loss: 1.7258 - val_accuracy: 0.2512 - 112s/epoch - 159ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.25195\n",
      "706/706 - 111s - loss: 1.7610 - accuracy: 0.2509 - val_loss: 1.7060 - val_accuracy: 0.2514 - 111s/epoch - 157ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.25195\n",
      "706/706 - 114s - loss: 1.7561 - accuracy: 0.2512 - val_loss: 1.7331 - val_accuracy: 0.2514 - 114s/epoch - 162ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.25195\n",
      "706/706 - 113s - loss: 1.7607 - accuracy: 0.2510 - val_loss: 1.7256 - val_accuracy: 0.2509 - 113s/epoch - 160ms/step\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "# from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# # Variables\n",
    "# train_folder = 'train'\n",
    "# num_classes = 7\n",
    "# image_size = 128\n",
    "# batch_size = 32\n",
    "# epochs = 10\n",
    "\n",
    "# # Data augmentation\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "#     validation_split=0.2  # 20% for validation\n",
    "# )\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_folder,\n",
    "#     target_size=(image_size, image_size),\n",
    "#     color_mode='rgb',\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='training'\n",
    "# )\n",
    "\n",
    "# validation_generator = train_datagen.flow_from_directory(\n",
    "#     train_folder,\n",
    "#     target_size=(image_size, image_size),\n",
    "#     color_mode='rgb',\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='validation'\n",
    "# )\n",
    "\n",
    "# # Load pre-trained MobileNetV2\n",
    "# mobile_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "# # Freeze base layers\n",
    "# for layer in mobile_base.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Custom top layers\n",
    "# x = mobile_base.output\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# # Create the final model\n",
    "# model = Model(inputs=mobile_base.input, outputs=predictions)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# # Set up callbacks\n",
    "# checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=train_generator.samples // batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=validation_generator.samples // batch_size,\n",
    "#     callbacks=[checkpoint, early_stopping],\n",
    "#     verbose=2\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22607 images belonging to 7 classes.\n",
      "Found 5649 images belonging to 7 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 21:46:38.735452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-05 21:48:13.624806: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25089, saving model to best_model_updated.h5\n",
      "706/706 - 117s - loss: 1.7617 - accuracy: 0.2510 - val_loss: 1.7356 - val_accuracy: 0.2509 - 117s/epoch - 166ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.25089 to 0.25124, saving model to best_model_updated.h5\n",
      "706/706 - 114s - loss: 1.7563 - accuracy: 0.2512 - val_loss: 1.7565 - val_accuracy: 0.2512 - 114s/epoch - 162ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.25124\n",
      "706/706 - 114s - loss: 1.7568 - accuracy: 0.2510 - val_loss: 1.7132 - val_accuracy: 0.2509 - 114s/epoch - 162ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.25124\n",
      "706/706 - 111s - loss: 1.7571 - accuracy: 0.2509 - val_loss: 1.7047 - val_accuracy: 0.2511 - 111s/epoch - 158ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.25124\n",
      "706/706 - 112s - loss: 1.7583 - accuracy: 0.2509 - val_loss: 1.7154 - val_accuracy: 0.2511 - 112s/epoch - 158ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.25124\n",
      "706/706 - 115s - loss: 1.7612 - accuracy: 0.2511 - val_loss: 1.7265 - val_accuracy: 0.2511 - 115s/epoch - 163ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.25124 to 0.25142, saving model to best_model_updated.h5\n",
      "706/706 - 112s - loss: 1.7568 - accuracy: 0.2512 - val_loss: 1.7126 - val_accuracy: 0.2514 - 112s/epoch - 159ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.25142\n",
      "706/706 - 114s - loss: 1.7529 - accuracy: 0.2512 - val_loss: 1.7227 - val_accuracy: 0.2514 - 114s/epoch - 161ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.25142\n",
      "706/706 - 113s - loss: 1.7514 - accuracy: 0.2511 - val_loss: 1.7036 - val_accuracy: 0.2509 - 113s/epoch - 160ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.25142\n",
      "706/706 - 112s - loss: 1.7535 - accuracy: 0.2511 - val_loss: 1.7107 - val_accuracy: 0.2514 - 112s/epoch - 159ms/step\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the previously saved model\n",
    "# model = load_model('best_model.h5')\n",
    "\n",
    "# # Update train_generator with the new data in the train/ folder\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_folder,\n",
    "#     target_size=(image_size, image_size),\n",
    "#     color_mode='rgb',\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='training'\n",
    "# )\n",
    "\n",
    "# # Update validation_generator with the new data in the train/ folder\n",
    "# validation_generator = train_datagen.flow_from_directory(\n",
    "#     train_folder,\n",
    "#     target_size=(image_size, image_size),\n",
    "#     color_mode='rgb',\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='validation'\n",
    "# )\n",
    "\n",
    "# # Set up callbacks\n",
    "# checkpoint = ModelCheckpoint('best_model_updated.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Train the model with the new data\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=train_generator.samples // batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=validation_generator.samples // batch_size,\n",
    "#     callbacks=[checkpoint, early_stopping],\n",
    "#     verbose=2\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
