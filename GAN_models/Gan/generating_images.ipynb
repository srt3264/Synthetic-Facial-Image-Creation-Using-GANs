{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path to the folder containing the training images\n",
    "train_path = 'data/train/'\n",
    "\n",
    "# Define the image size and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # rescale pixel values to [0, 1]\n",
    "    rotation_range=20, # randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1, # randomly shift the image horizontally by up to 10%\n",
    "    height_shift_range=0.1, # randomly shift the image vertically by up to 10%\n",
    "    horizontal_flip=True, # randomly flip images horizontally\n",
    "    zoom_range=0.1, # randomly zoom images by up to 10%\n",
    ")\n",
    "\n",
    "# Create a flow_from_directory generator for the training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path, # path to the training data folder\n",
    "    target_size=(img_height, img_width), # resize the images to the specified size\n",
    "    batch_size=batch_size, # set the batch size\n",
    "    class_mode='categorical', # set the class mode to categorical\n",
    "    shuffle=True, # shuffle the data for training\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "# test data\n",
    "test_path = 'data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Load the original dataset\n",
    "# Replace this with your own dataset\n",
    "\n",
    "\n",
    "# Normalize the pixel values to the range [-1, 1]\n",
    "X_train = (train_generator[0][0].astype(np.float32) - 0.5) * 2\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(7*7*128, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Conv2D(1, (7,7), activation='tanh', padding='same'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator(img_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the generator and discriminator\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the image shape and latent dimension\n",
    "img_shape = (28, 28, 1)\n",
    "latent_dim = 100\n",
    "\n",
    "# Build the generator, discriminator, and GAN models\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Compile the discriminator with binary crossentropy loss and Adam optimizer\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Compile the GAN with binary crossentropy loss and Adam optimizer\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Generate random noise vectors\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Generate fake images using the generator\n",
    "    gen_images = generator.predict(noise)\n",
    "\n",
    "    # Get a batch of real images from the dataset\n",
    "    real_images = X_train[np.random.randint(0, X_train.shape[0], batch_size)]\n",
    "\n",
    "    # Concatenate the real and fake images\n",
    "    images = np.concatenate([real_images, gen_images])\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real_labels = np.ones((batch_size, 1))\n",
    "    fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Label smoothing for real images\n",
    "    real_labels *= 0.9 + np.random.random(real_labels.shape) * 0.1\n",
    "\n",
    "    # Train the discriminator on the real and fake images\n",
    "    d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_images, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train the generator by trying to fool the discriminator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    # Print the progress\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Discriminator Loss: {d_loss[0]}, Discriminator Accuracy: {100*d_loss[1]}, Generator Loss: {g_loss}\")\n",
    "       # Save a sample of the generated images every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        gen_images = generator.predict(np.random.normal(0, 1, (25, latent_dim)))\n",
    "        gen_images = 0.5 * gen_images + 0.5 # Rescale to [0, 1]\n",
    "        fig, axs = plt.subplots(5, 5)\n",
    "        cnt = 0\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                axs[i,j].imshow(gen_images[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(f\"gan_images_epoch{epoch+1}.png\")\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
